\section*{Linear Regression}
$y_i = \beta_1 x_{i1}+...+\beta_p x_{ip} + \epsilon_i$ ($x_{i1}\equiv 1$, so $\beta_1$ is intercept; $Y$: response var., $x$: predictor. $n$: sample size; $p$: number of predictors).
Assumptions: $\epsilon_1,...,\epsilon_n$ independent (uncorrelated), $E(\epsilon_1)=0, \text{Var} (\epsilon_i)=\sigma^2$ (unknown; homoscedasticity)
Matrix Form: $\stackrel{n\times 1}{Y}= \stackrel{n\times p}{X} \cdot \stackrel{p\times 1}{\beta} + \stackrel{n\times 1}{\epsilon}$. $X=(x_{ij})_{i,j}$ is called design matrix.

\textbf{LSE:} $\hat \beta = \text{argmin}_\beta ||Y-Xb||_2^2 = (X^\top X)^{-1}X^\top Y \sim \mathcal{N}_p(\beta, \sigma^2 (X^\top X)^{-1})$

$\hat \sigma ^2=\tfrac{1}{n-p}\sum_{i=1}^n(Y_i-(X\hat\beta)_i)^2= \tfrac{1}{n-p} RSS$.
Then $\Ex[\hat\sigma^2]=\sigma^2$ (unbiased).
Moreover, $\Ex[\hat\beta]=\beta$, $\Var[\hat\beta]= \sigma^2(X^TX)^{-1}$.
Thus if $\epsilon \sim N_p(0, \sigma^2 Id)$, then $\hat\beta \sim N_p(\beta, \sigma^2(X^TX)^{-1})$.
% Thus estimated variance $\widehat{\Var}(\hat\beta_j)=\left[\hat \sigma^2(X^TX)^{-1}\right]_{jj}$. % repeated below @ measuring goodness of fit
Also, if error Gaussian, then: $\hat Y \sim \mathcal{N}_n(X\beta, \sigma^2 P)$, error

$e \sim \mathcal{N}_n(0, \sigma^2(I-P))$,
$\hat \sigma^2 \sim {\sigma^2 }/{(n-p)}\cdot X_{n-p}$
where $P=X(X^\top X)^{-1}X^\top $

\textbf{Categorical Variables:}
For two levels:$y_i = \beta_1 x_{i1}+...+\beta_p x_{ip} + \lambda d_{is} + \epsilon_i$
so if $i$ is in category, then $d_{is}=1$ else $d_{is}=0$. This acts as a different intercept ($E(y_i)-E(y_j)=\lambda$).
If more categories, add more dummy variables ($n-1$ for $n$ levels).
% 
\textbf{Interaction:} dummy can also influence slope:
add term $\delta d_i x_i$, can influence interaction between predictors:
add term $\delta x_{i2} x_{i3}$, can influence other categorical variable:
add term $\delta d_{i1}d_{i2}$.
Product of two predictors is also a predictor.
In case of one categorical variable and one quantitative predictor, this leads to different slopes of the planes.
But also other combinations possible.

\subsection*{Measuring Goodness of Fit}
Proportion of variance that is explained by fitted linear model: $R^2=1-\sfrac{RSS}{TSS}\in [0,1]$,
where $TSS= \sum_{i=1}^n(y_i-\bar y)^2$, $RSS= \sum_{i=1}^n(y_i-\hat y_i)^2$. 
% remaining variance $= \sfrac{RSS/(n-1)}{TSS/(n-1)}$.
In simple lin. reg.: $R^2=r^2$, sample correlation.
To account for more variables in model ($R^2$ decreases), $\textup{adj}R^2 = 1 - \sfrac{RSS/(n-p)}{TSS/(n-1)}$.

$H_0: y = X\beta + \epsilon$ with $\beta_j=0$
$H_A: y = X\beta + \epsilon$ with $\beta_j\neq 0$

Under $H_0: \sfrac{\hat \beta_j - (E[\hat \beta_j]=0)}{\sqrt{\sigma^2(X^\top X)^{-1}_{jj}}} \sim \mathcal{N}(0,1)$,
t-statistic: $\sfrac{\hat \beta_j}{\sqrt{\widehat{\Var}[\hat\beta_j]}} \sim t_{n-p}$

Test for significance of $j$-th predictor. 
$H_0$: $Y=X\beta+\epsilon$ with $\beta_j=0$, 
$H_1$: $Y=X\beta+\epsilon$ with $\beta_j\neq 0$.
Under $H_0$: $\hat\beta_j \sim N(0, \left[\sigma^2(X^TX)^{-1}\right]_{jj})$. 
Thus $\sfrac{\hat\beta_j}{\sqrt{\Var[\hat\beta_j]}}\sim N(0,1)$ 
and $\sfrac{\hat\beta_j}{\sqrt{\widehat{\Var}[\hat\beta_j]}}\sim t_{n-p}$.
%
Remark: $\widehat{\Var}[\hat\beta_j]=\left[\hat \sigma^2(X^TX)^{-1}\right]_{jj} = (se(\hat\beta))^2 = \tfrac{\hat\sigma^2}{(n-1)\Var(X_j)}\tfrac{1}{1-R_j^2}$, where $R_j^2$ is the multiple $R^2$ from regression of $X_j$ on all other predictors.
One finds that an $1-\alpha$-C.I. for $\beta_j$ is $\hat\beta_j\pm \widehat{\se}({\hat\beta_j})t_{1-\alpha/2, n-p}$. Let $X_0=(x_{01},...,X{0p}$ be a new point.
Then a $1-\alpha$-C.I. for $\Ex[Y_0]$ is $X_0^T\hat\beta \pm \hat \sigma\sqrt{X_0^T(X^TX)^{-1}X_0} t_{1-\alpha/2, n-p}$. A $1-\alpha$-C.I. (prediction Interval) for $Y_0$ is $X_0^T\hat\beta \pm \hat \sigma\sqrt{1+X_0^T(X^TX)^{-1}X_0} t_{1-\alpha/2, n-p}$.
%
Note: $\Var(AY)=A\cdot\Var(X)A^T$.

\begin{codebox}{r}{Linear Regression}
  fit <- lm(y~x1+x2) # Fit only x1 and x2 (so p=3)
  predict(fit, pred.data.frame)
  X <- cbind(1, x1, x2) # p = 3
  XtX.inv <- solve(t(X) %*% X) # Manual fit
  beta.hat <- XtX.inv %*% t(X.int) %*% y
  res <- y - X.int %*% beta.hat # Residuals
  RSE <- sqrt(sum(res^2)/(n-p)) # Residual std. error: Est. of the sd of the noise in the linear model
  se.x1 <- RSE * sqrt(XtX.inv[2, 2]) # Std. error of x1
  t.val.x1 <- beta.hat[2] / se.x1 # T value of x1
  p.val.x1 <- 2*pt(abs(t.val.x1), df=n-p, lower=F)
  RSE <- sqrt(sum(residuals(fit)^2)/(n-p))
  RSS <- sum(res^2) # Residual sum of squares
  TSS <- sum((y - mean(y))^2) # Total sum of squares
  R.sq <- 1 - RSS / TSS # R^2
  AdjR2 <- 1 - (RSS/(n-p))/(TSS/(n-1))
  # Alternative t-value
  coef <- summary(fit1)$coefficients
    t1 <- coef["x1","Estimate"]/coef["x1","Std. Error"]
    # Finding p-values
    fit.smaller <- lm(y ~ x1)
    anova(fit.smaller, fit, fit.all)
    # Overall F-Test
    fit.empty <- lm(y ~ 1, data=...) # Empty model
    anova(fit.empty, fit) # Compare models
    # Alternative F-test
    Ftest <- summary(fit)$fstatistic
  pval <- 1 - pf(Ftest[1], df1=Ftest[2], df2=Ftest[3])
  # Categ. var. by hand & LOOCV
  a1 <- (levels(shelveloc)[2]==shelveloc)*1
  lcv<-mean((residuals(fit)/(1-lm.influence(fit)$h))^2)
\end{codebox}

\textbf{R Diagnostic plots: }
\#1 Tukey-Anscombe Plot \texttt{plot(fit, which=1)} the points follow the line, else $E(\epsilon)=0$ violated.
\#2 Q-Q Plot should follow line, else error not Gaussian (still all fine).
\#3 Scale-Location: should be flat, else $\text{Var}(\epsilon_i)=\sigma^2$ violated (p-values wrong).
\#4/\#5 Cook distance: shows if some data points have a larger impact on the fit than others (outliers).
Note: cannot detect if the residuals are correlated with these plots!
