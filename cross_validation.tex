\section*{Cross Validation}
Can be used for \textit{model assessment} (estimate test MSE) and \textit{model selection} (choose tuning parameters, variable selection). 
But not both at the same time (use double CV instead).

\textbf{Validation set:} split data into two halves, train on one, test on the other (most bias).
Pros: "fair" estimate of test MSE.
Cons: Too pessimistic (because only trained on half of the data), thus biased estimate.
Varies a lot depending on split, large variance.
\textbf{k-Fold:} same, but with many folds.
Try all folds for test and average metrics over the folds (in between). $\text{Var}(\hat {\theta_k}) = 1/K \cdot \hat {\text{Var}}(MSEs)$
K-fold CV estimator: 
$\frac{1}{K}\sum_{k=1}^K MSE_k$ with $MSE_k = \frac{1}{|F_k|}\sum_{i\in F_k} (y_i-\hat f^{(-k)}(x_i))^2$.

\textbf{Leave one out cross validation (LOOCV):} extreme version where each data point is a fold (least bias).
For $i=1,...,n$: train $\hat f$ on $(x_j,y_j)$, $j\in \{1,...,n\}\setminus \{i\}$; 
evaluate $\hat f$ at $x_i$ $\rightarrow \hat f^{(-i)}(x_i)$; $MSE_i=(y_i-\hat f^{(-i)}(x_i))^2$.
LOOCV estimator of expected test MSE is $\sum_{i=1}^n MSE_i$.

$\theta_k = \tfrac 1 k \sum_{i=1}^k \tfrac 1 {|I_k|} \sum_{i\in I_k} (y_i - \hat f^{-I_k}(x_i))^2$,
$\theta_{L} = \tfrac 1 n \sum_{i=1}^n (y_i - \hat f^{-i}(x_i))^2$

Pros: Less biased, no randomness due to split. 
Cons: computationally intensive (but sometimes shortcut).
