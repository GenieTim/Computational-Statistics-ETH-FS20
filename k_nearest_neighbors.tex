\section*{K Nearest Neighbors}
Non-parametric method: $\hat f(x_0)= \tfrac{1}{k} \sum_{x_i\in N_0} y_i$, where $N_0$ is the set of k training observations with $x$-values closest to $x_0$.
If $k$ is larger has less variance, more bias. If $k$ is smaller has more variance, less bias.
Fails if there are lots of useless predictors.

\textbf{LOESS smoother}
Similar, but smooth weight function, $\alpha$ controls smoothing.
$\alpha$ small means less smoothing. See also: GAM

\begin{codebox}{r}{KNN \& LOESS}
   library(kknn)
   dfTrain=data.frame(y=Ytrain,x=Xtrain)
   dfTest=data.frame(x=Xtest)
   fit.kknn <- kknn(y ~ ., dfTrain,dfTest,k=8)
   predTest=predict(fit.kknn) # predictions on dfTest
   library(class) # Alternative library for knn
   knn(train, test, k=5)
   lo <- loess(y ~ x, span=alpha) # for loess (smoother)
   prediction <- predict(object=lo, x)
\end{codebox}
