\section*{Model Selection}
Criteria for model selection (for linear models): 
$\text{Mallow's }C_p=\tfrac 1 n (RSS + 2d\cdot \hat\sigma^2)$, 
$AIC = \tfrac{1}{n\hat\sigma^2}(RSS+2d\cdot\hat\sigma^2)$, 
$BIC= \tfrac{1}{n\hat\sigma^2}(RSS+\log(n)d\cdot\hat\sigma^2) = -2 \cdot\log(\hat L) + d\cdot\log(n)$, 
where $\hat L$ is the maximized value of the likelihood of the model, 
$n$: sample size, $d$: nr. of params in model, $AdjR^2=1-\tfrac{RSS / (n-d-1)}{TSS / (n-1)}$.

\subsection*{Shrinkage Methods}
Assume centered variable (no intercept).
Remark: Ridge does not really do model selection, Lasso does, because $L^1$-spheres have "corners".
Note: first standartize variables to have variance 1: $\tilde x_{ij}=\sfrac{x_{ij}}{\sqrt{\tfrac{1}{n} \sum_i(x_{ij}- \bar x_j)^2}}$
\textbf{Ridge regression:} $\hat\beta_s^{ridge} = \text{argmin}_\beta RSS(\beta) + \lambda ||\beta||_2^2 = (X^\top X + \lambda I)^{-1}X^\top y$ (if $X$- matrix has no intercept (i.e. center variables before))
\textbf{Lasso:} $\hat\beta_s^{lasso}\text{argmin}_\beta RSS(\beta) + \lambda ||\beta||_1$,
\textbf{Elastic Net:} $\hat\beta_s^{elastic} = \text{argmin}_\beta RSS(\beta)+(1-\alpha)\lambda||\beta||_1 + \alpha \lambda ||\beta||_2^2$($\alpha=1$: ridge, $\alpha=0$: lasso),

\textbf{Adaptive Lasso:} Lasso with penalty weights:

$\hat\beta_s^{adapt.lasso}\text{argmin}_\beta RSS(\beta) + \lambda \sum_{j=1}^p w_j |\beta_j|$. Take a $\sqrt{n}$ consistent estimate $\hat\beta$ of $\beta$ (e.g. least squares Choose $\gamma>0$, then set $\hat w_j = {1}/{|\hat\beta|^\gamma}$.
This asymptotically selects the right covariates and has optimal estimation rate.

\textbf{Group Lasso:} Predictors are divided into $L$ groups of size $p_1, ..., p_L$s.t. $\sum p_i = p$. $\hat\beta_\lambda^{gr.lasso}=\text{argmin}_\beta RSS(\beta)+\lambda \sum_{l=1}^L \sqrt{p_l} ||\beta||_2$. (if L=p, we get Lasso).
Acts like Lasso on a group level.
Useful if there are categorical variables with $>2$ categories (put all corresponding dummy variables in a group).

\subsection*{Best subset selection:}
$\hat\beta_s^{subset} = \text{argmin}_{\beta, ||\beta||_0 \leq s} RSS(\beta)$. 1) Fit $M_0$ (null model) = sample mean. 2) For $k=1,...,p$ fit $\binom p k$models that contain exactly $k$ predictors and select best (smallest RSS): $M_k$. 3) Select best among $M_0, ..., M_p$ using CV or criteria.

1. Let $\mathcal{M}_0$ denote the \textit{null model}, which contains no predictors (i.e. simply predicts the sample mean).

2. For $k=1,2,...,p$: (a) Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors. (b) Pick the best among these $\binom{p}{k}$ models, call it $\mathcal{M}_k$, i.e. having the smallest $RSS$ (equiv. largest $R^2$)

3. Select a single best model from among $\mathcal{M}_0,..., \mathcal{M}_p$, using CV, $C_p$ (AIC), BIC, or adj. $R^2$.

To reduce computational cost, do in step 2. forward/backward stepwise selection ($\rightarrow O(p^2)$ models to compare).

\textbf{Forward stepwise:} 
1) Fit $M_0$ 
2) For $k=0,...,p-1$ fit all $p-k$ models with 1 additional predictor and select best (smallest RSS): $M_k$. 
3) Select best among $M_0,....,M_p$ using CV or criteria.

\textbf{Backward stepwise:} 
1) Fit $M_p$ (full model). 
2) For $k=p,p-1,...,1$: fit all $k$ models that drop one perdictor in $M_k$. Choose best (smallest RSS): $M_{k-1}$. 
3) Select best among $M_0,...,M_p$ using CV or criteria.

\begin{codebox}{r}{Model Selection}
  # Lasso/Ridge
  library(glmnet)
  grid <- 10^seq(from=10,to=-2,length=100)
  # x must be matrix, e.g. model.matrix(Salary . , Hitters)[, -1]
  ridge <- glmnet(x[train,], y[train], alpha=0, lambda=grid)
  lasso <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
  # Predict for new $\lambda = 35$ and for new data use
  predict(ridge.mod, s=35, type="coefficients", newx=x[test,])
  # for CV to choose $\lambda$
  c <- cv.glmnet(x[train,], y[train], alpha=0, nfolds=10)
  c$lambda.min # gives best lambda. Plot coefficient paths: plot(fit.lasso)
  # Best subset. method="forward" or "backward" for stepwise
  regfit.full=regsubsets(Salary~., data=..., nvmax=19)
  # Mallow Cp
  library(leaps)
  m <- leaps::regsubsets(y~., data=train, nvmax=10)
  mo <- which.min(summary(m)$cp)
  form <- as.formula(paste("y~",
  paste(names(coef(m,mo))[-1],collapse="+"), sep = ""))
  fit <- lm(form, data=test)
  # Workaround for categorical variables
  predict.regsubsets <- function(reg, new.data, id) {
    form <- as.formula(reg$call[[2]])
    mat <- model.matrix(form, new.data)
    coefi <- coef(reg, id=id)
    return(mat[,names(coefi)]%*%coefi)
  }
  n <- nrow(data)
  folds <- sample(cut(1:n, 10, labels=F), n, replace=F)
  for (fold in 1:10) {
  test.fold <- which(folds == fold)
  data.train <- data[-test.fold,]
  data.test <- data[test.fold,]
  m <- nrow(data.train)
  cv.f <- sample(cut(1:m, 10, labels=F), m, replace=F)
  cv.errors <- matrix(nrow=10, ncol=19)
  for (k in 1:10) {
    cv.tf <- (cv.f == k)
    cv.m <- regsubsets(Salary~., data.train[-cv.tf,], nvmax=19)
    for (i in 1:19) {
    pred <- predict(cv.m, data.train[cv.tf,], id=i)
    cv.errors[k, i] <- mean((pred-data.train[cv.tf,]$Salary)^2)
    }
  }
  m <- regsubsets(Salary~., data=data.train, nvmax=19)
  best.cp <- which.min(summary(m)$cp)
  best.cv <- which.min(apply(cv.errors, 2, mean))
  pred.cp <- predict.regsubsets(m, data.test, best.cp)
  pred.cv <- predict.regsubsets(m, data.test, best.cv)
  }
  # For double CV with glmnet, can use cv.glmnet
  inner.folds <- factors(folds[folds!=i])
  levels(inner.folds) <- 1:(k-1)
  inner.folds <- as.numeric(inner.folds)
\end{codebox}
